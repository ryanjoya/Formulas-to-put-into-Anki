Naive Bayes. Classification is a natural first step to machine learning and the Naive Bayes classifier combines that with the trending concept of Bayesian inference. Despite its simplicity, can address key problems like spam classification.

Map-Reduce. A simple and visual way of leveraging multiple computers to solve a single task. More on its different uses here: MapReduce Patterns, Algorithms, and Use Cases.

Hill Climbing. Introduces optimization over a non-exhaustible domain and leads to a whole array of powerful methods like gradient-descent, simulated annealing. Optimization allows us to do decision making, it's important.

Binary search. Introduces logarithmic complexity and how the structure of data can change complexity characteristics of operations.

Exponentiation (powers) by halving. Great example of a recursive attack. You would be surprised how many problems can be described as exponentiation (for an associative operation) of something: a number, a matrix and more. Again it's magical that associativity of an operation allows you to turn a linear factor into a logarithmic one.

Floyd-Warshall. Graphs as matrices, shortest paths, negative cycles, dynamic programming in five lines of code.

Inductive inference. Inductive inference. This idea on how to find programs that do certain things without having a human write them I believe is the key to the future of AI and simulating and understanding the human mind.

Markov Chain Monte Carlo. Approximate integral, expected value computation based on a conditional specification of the probability distribution. This gives us a general way of doing data analysis.

Shor's algorithm. Non-trvial example of a quantum algorithm that shows a significant complexity improvement. Reveals how quantum computing works.

PageRank. How to iteratively solve for tightly related variables to extract metrics on graphs.
